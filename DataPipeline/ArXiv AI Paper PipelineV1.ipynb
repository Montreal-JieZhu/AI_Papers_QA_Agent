{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85c1e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from dataclasses import dataclass, asdict\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Iterable, List, Dict, Any\n",
    "\n",
    "import fitz  # PyMuPDF\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "# pip install apscheduler\n",
    "from apscheduler.schedulers.blocking import BlockingScheduler\n",
    "from apscheduler.triggers.cron import CronTrigger\n",
    "from tzlocal import get_localzone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5f0035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------- Configuration ---------------------------- #\n",
    "\n",
    "class Config:\n",
    "    \"\"\"\n",
    "    Adjust these defaults to fit your environment and preferences.\n",
    "    \"\"\"\n",
    "\n",
    "    # Source: arXiv search results list page (HTML). It always show the papers ordered by the announcement date desc.\n",
    "    SEARCH_URL: str = (\n",
    "        \"https://arxiv.org/search/?query=cs.ai&searchtype=all&abstracts=show&order=-announced_date_first&size=50\"\n",
    "    )\n",
    "\n",
    "    # Base working directory\n",
    "    BASE_DIR: Path = Path(\"./paper\")\n",
    "\n",
    "    # Metadata files: base.json has all metadata of loaded papers. arxiv_search_result.json only has the latest 50 paper's metadata\n",
    "    SCRAPE_OUTFILE: Path = BASE_DIR / \"arxiv_search_result.json\"\n",
    "    BASE_METADATA_FILE: Path = BASE_DIR / \"base.json\"\n",
    "\n",
    "    # Artifacts: The paths to pdf files and txt files. But those files will be deleted, if the text are loaded into all.txt.\n",
    "    PDF_DIR: Path = BASE_DIR / \"pdf\"\n",
    "    TXT_DIR: Path = BASE_DIR / \"txt\"\n",
    "    CONCATENATED_TXT: Path = TXT_DIR / \"all.txt\"\n",
    "\n",
    "    # Behavior Variables: In order the paper provider doesn't prevent us from downloading papers.    \n",
    "    REQUEST_TIMEOUT_SEC: int = 20\n",
    "    REQUESTS_PER_SECOND: float = 1.5  # polite crawling: ~1 req / 0.67 s\n",
    "\n",
    "    # HTTP retry/backoff\n",
    "    RETRY_TOTAL: int = 5\n",
    "    RETRY_BACKOFF_FACTOR: float = 0.5  # exponential backoff base\n",
    "\n",
    "    # Logging\n",
    "    LOG_LEVEL: int = logging.INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77ebe42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------ Data Model ----------------------------- #\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Paper:\n",
    "    \"\"\"Normalized paper metadata record.\n",
    "\n",
    "    `identity_key` is used for deduplication and should be stable across runs.\n",
    "    We derive it from the arXiv ABS URL (preferred) or the PDF URL.\n",
    "    \"\"\"\n",
    "\n",
    "    title: str\n",
    "    abs_url: str\n",
    "    pdf_url: str\n",
    "    authors: List[str]\n",
    "    abstract: str\n",
    "    submitted_date_raw: str  # original raw text, e.g. \"Submitted 12 Aug 2025\"\n",
    "    identity_key: str  # canonical key used to detect duplicates\n",
    "\n",
    "    @staticmethod\n",
    "    def from_scraped(\n",
    "        title: str,\n",
    "        abs_url: str,\n",
    "        pdf_url: str,\n",
    "        authors: List[str],\n",
    "        abstract: str,\n",
    "        submitted_date_raw: str,\n",
    "    ) -> \"Paper\":\n",
    "        key = _derive_identity_key(abs_url, pdf_url)\n",
    "        return Paper(\n",
    "            title=title,\n",
    "            abs_url=abs_url,\n",
    "            pdf_url=pdf_url,\n",
    "            authors=authors,\n",
    "            abstract=abstract,\n",
    "            submitted_date_raw=submitted_date_raw,\n",
    "            identity_key=key,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c21d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------- Logging -------------------------------- #\n",
    "\n",
    "def setup_logging(level: int = Config.LOG_LEVEL) -> None:\n",
    "    logging.basicConfig(\n",
    "        level=level,\n",
    "        format=\"%(asctime)s | %(levelname)s | %(message)s\",\n",
    "        datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe056e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------- HTTP Utilities --------------------------- #\n",
    "\n",
    "def build_http_session(cfg: Config = Config) -> requests.Session:\n",
    "    \"\"\"Create a `requests` session with retry/backoff and helpful headers.\"\"\"\n",
    "    session = requests.Session()\n",
    "\n",
    "    retry = Retry(\n",
    "        total=cfg.RETRY_TOTAL,\n",
    "        read=cfg.RETRY_TOTAL,\n",
    "        connect=cfg.RETRY_TOTAL,\n",
    "        backoff_factor=cfg.RETRY_BACKOFF_FACTOR,\n",
    "        status_forcelist=(500, 502, 503, 504),\n",
    "        allowed_methods=(\"GET\",),\n",
    "        raise_on_status=False,\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount(\"http://\", adapter)\n",
    "    session.mount(\"https://\", adapter)\n",
    "    session.headers.update(\n",
    "        {\n",
    "            \"User-Agent\": (\n",
    "                \"arxiv-pipeline/1.0 (+https://github.com/your-org/your-repo)\"\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "    return session\n",
    "\n",
    "\n",
    "def polite_sleep(cfg: Config = Config) -> None:\n",
    "    \"\"\"Respectful delay between requests to avoid hammering arXiv.\"\"\"\n",
    "    time.sleep(max(0.0, 1.0 / cfg.REQUESTS_PER_SECOND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c386d3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------- Scraping / Extraction ------------------------ #\n",
    "\n",
    "def fetch_search_metadata(search_url: str, session: requests.Session) -> List[Paper]:\n",
    "    \"\"\"Fetch and parse paper metadata from an arXiv search results page.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    search_url : str\n",
    "        arXiv search results URL (with your query params already set).\n",
    "    session : requests.Session\n",
    "        Configured HTTP session with retry/backoff.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[Paper]\n",
    "        List of normalized paper metadata entries.\n",
    "    \"\"\"\n",
    "    logging.info(\"Fetching search results: %s\", search_url)\n",
    "    resp = session.get(search_url, timeout=Config.REQUEST_TIMEOUT_SEC)\n",
    "    resp.raise_for_status()\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "    papers: List[Paper] = []\n",
    "    results = soup.find_all(\"li\", class_=\"arxiv-result\")\n",
    "    logging.info(\"Found %d result items\", len(results))\n",
    "\n",
    "    for item in results:\n",
    "        try:\n",
    "            title_tag = item.find(\"p\", class_=\"title\")\n",
    "            title = (title_tag.get_text(strip=True) if title_tag else \"\").strip()\n",
    "\n",
    "            list_title = item.find(\"p\", class_=\"list-title\")\n",
    "            # Prefer ABS link (canonical ID holder)\n",
    "            abs_a = list_title.find(\"a\") if list_title else None\n",
    "            abs_url = abs_a[\"href\"].strip() if abs_a and abs_a.has_attr(\"href\") else \"\"\n",
    "\n",
    "            # A separate link for PDF often exists; if not, derive from ABS link\n",
    "            pdf_a = (list_title.find(\"a\", string=re.compile(r\"^pdf$\", re.I)) if list_title else None)\n",
    "            pdf_url = (\n",
    "                pdf_a[\"href\"].strip()\n",
    "                if pdf_a and pdf_a.has_attr(\"href\")\n",
    "                else _try_abs_to_pdf(abs_url)\n",
    "            )\n",
    "\n",
    "            authors_p = item.find(\"p\", class_=\"authors\")\n",
    "            authors = [a.get_text(strip=True) for a in authors_p.find_all(\"a\")] if authors_p else []\n",
    "\n",
    "            abstract_span = item.find(\"span\", class_=\"abstract-full\")\n",
    "            abstract = abstract_span.get_text(strip=True) if abstract_span else \"\"\n",
    "\n",
    "            date_p = item.find(\"p\", class_=\"is-size-7\")\n",
    "            submitted_raw = (\n",
    "                date_p.get_text(\" \", strip=True) if date_p else \"\"\n",
    "            )\n",
    "\n",
    "            paper = Paper.from_scraped(\n",
    "                title=title,\n",
    "                abs_url=abs_url,\n",
    "                pdf_url=pdf_url,\n",
    "                authors=authors,\n",
    "                abstract=abstract,\n",
    "                submitted_date_raw=submitted_raw,\n",
    "            )\n",
    "            papers.append(paper)\n",
    "        except Exception as e:\n",
    "            logging.warning(\"Error parsing a search result: %s\", e, exc_info=False)\n",
    "            continue\n",
    "\n",
    "    return papers\n",
    "\n",
    "\n",
    "def _try_abs_to_pdf(abs_url: str) -> str:\n",
    "    \"\"\"Best effort to convert an arXiv ABS URL to a PDF URL.\"\"\"\n",
    "    # ABS: https://arxiv.org/abs/2501.12345v2  ->  PDF: https://arxiv.org/pdf/2501.12345.pdf\n",
    "    m = re.search(r\"/abs/(\\d+\\.\\d+)(v\\d+)?$\", abs_url)\n",
    "    if m:\n",
    "        return f\"https://arxiv.org/pdf/{m.group(1)}.pdf\"\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def _derive_identity_key(abs_url: str, pdf_url: str) -> str:\n",
    "    \"\"\"Compute a stable identity key for a paper.\n",
    "\n",
    "    Prefer the arXiv numeric id (without version) extracted from the ABS URL.\n",
    "    Fallback to the numeric id extracted from the PDF URL.\n",
    "    \"\"\"\n",
    "    for url in (abs_url, pdf_url):\n",
    "        m = re.search(r\"/(?:abs|pdf)/(\\d+\\.\\d+)\", url or \"\")\n",
    "        if m:\n",
    "            return m.group(1)  # e.g., \"2501.12345\"\n",
    "    # Last resort: use the URL itself (less stable but better than nothing)\n",
    "    return abs_url or pdf_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36174dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------- Files / Persistence -------------------------- #\n",
    "\n",
    "def ensure_dirs(*paths: Path) -> None:\n",
    "    for p in paths:\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def load_json_list(path: Path) -> List[Dict[str, Any]]:\n",
    "    if not path.exists():\n",
    "        return []\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def atomic_write_json(path: Path, data: Any) -> None:\n",
    "    tmp = path.with_suffix(path.suffix + \".tmp\")\n",
    "    with tmp.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    os.replace(tmp, path)\n",
    "\n",
    "\n",
    "def safe_filename(name: str, extra_key: str = \"\") -> str:\n",
    "    \"\"\"Return a filesystem‑safe filename derived from `name` + optional key.\"\"\"\n",
    "    base = f\"{name} {extra_key}\".strip() if extra_key else name\n",
    "    # Collapse whitespace and remove disallowed characters\n",
    "    base = re.sub(r\"\\s+\", \" \", base).strip()\n",
    "    base = re.sub(r\"[\\\\/*?:\\\"<>|]\", \"_\", base)\n",
    "    # Truncate to avoid super‑long filenames on Windows/macOS\n",
    "    return base[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32f72d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------ Diff logic ----------------------------- #\n",
    "\n",
    "def compute_new_papers(\n",
    "    existing: List[Dict[str, Any]], fresh: List[Paper]\n",
    ") -> List[Paper]:\n",
    "    \"\"\"Return only papers whose identity_key is not in the existing base.\"\"\"\n",
    "    seen_keys = {rec.get(\"identity_key\") for rec in existing}\n",
    "    new_items = [p for p in fresh if p.identity_key not in seen_keys]\n",
    "    logging.info(\"%d new papers detected (out of %d fresh)\", len(new_items), len(fresh))\n",
    "    return new_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1b68ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------ Download ------------------------------- #\n",
    "\n",
    "def download_pdfs(papers: Iterable[Paper], session: requests.Session, cfg: Config = Config) -> None:\n",
    "    ensure_dirs(cfg.PDF_DIR)\n",
    "    for i, p in enumerate(papers, start=1):\n",
    "        if not p.pdf_url:\n",
    "            logging.warning(\"[%d] Missing PDF URL for: %s\", i, p.title)\n",
    "            continue\n",
    "        filename = safe_filename(p.title, extra_key=p.identity_key) + \".pdf\"\n",
    "        dest = cfg.PDF_DIR / filename\n",
    "        if dest.exists():\n",
    "            logging.info(\"[%d] Already exists, skipping: %s\", i, dest.name)\n",
    "            continue\n",
    "\n",
    "        logging.info(\"[%d] Downloading: %s\", i, p.pdf_url)\n",
    "        try:\n",
    "            resp = session.get(p.pdf_url, stream=True, timeout=cfg.REQUEST_TIMEOUT_SEC)\n",
    "            resp.raise_for_status()\n",
    "            tmp = dest.with_suffix(\".pdf.part\")\n",
    "            with tmp.open(\"wb\") as f:\n",
    "                for chunk in resp.iter_content(chunk_size=1024 * 64):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "            os.replace(tmp, dest)\n",
    "        except Exception as e:\n",
    "            logging.error(\"Failed to download %s | %s\", p.pdf_url, e)\n",
    "            # Clean partial file if present\n",
    "            try:\n",
    "                if tmp.exists():\n",
    "                    tmp.unlink(missing_ok=True)\n",
    "            except Exception:\n",
    "                pass\n",
    "        finally:\n",
    "            polite_sleep(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5478f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------- PDF → Text stage -------------------------- #\n",
    "\n",
    "def convert_pdfs_to_txt_and_cleanup(cfg: Config = Config) -> None:\n",
    "    ensure_dirs(cfg.TXT_DIR)\n",
    "\n",
    "    pdf_files = sorted(cfg.PDF_DIR.glob(\"*.pdf\"))\n",
    "    if not pdf_files:\n",
    "        logging.info(\"No PDFs to convert in %s\", cfg.PDF_DIR.resolve())\n",
    "        return\n",
    "\n",
    "    for i, pdf_path in enumerate(pdf_files, start=1):\n",
    "        txt_name = pdf_path.with_suffix(\".txt\").name\n",
    "        txt_path = cfg.TXT_DIR / txt_name\n",
    "        tmp_txt = txt_path.with_suffix(\".txt.part\")\n",
    "\n",
    "        logging.info(\"[%d/%d] Extracting text: %s\", i, len(pdf_files), pdf_path.name)\n",
    "        try:\n",
    "            doc = fitz.open(pdf_path)\n",
    "            try:\n",
    "                with tmp_txt.open(\"w\", encoding=\"utf-8\", newline=\"\\n\") as out:\n",
    "                    for page in doc:\n",
    "                        # `get_text()` default is the same as 'text'\n",
    "                        out.write(page.get_text())\n",
    "                os.replace(tmp_txt, txt_path)\n",
    "                logging.info(\"✅ Wrote %s\", txt_path.name)\n",
    "            finally:\n",
    "                doc.close()\n",
    "\n",
    "            pdf_path.unlink()\n",
    "        except Exception as e:\n",
    "            logging.error(\"Failed to convert %s: %s\", pdf_path.name, e)\n",
    "            try:\n",
    "                tmp_txt.unlink(missing_ok=True)\n",
    "            except Exception:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e0d36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------- Concatenate stage -------------------------- #\n",
    "\n",
    "def concatenate_txts(cfg: Config = Config) -> None:\n",
    "    files = sorted(p for p in cfg.TXT_DIR.glob(\"*.txt\") if p.name != cfg.CONCATENATED_TXT.name)\n",
    "    if not files:\n",
    "        logging.info(\"No TXT files to merge in %s\", cfg.TXT_DIR.resolve())\n",
    "        return\n",
    "\n",
    "    tmp_out = cfg.CONCATENATED_TXT.with_suffix(cfg.CONCATENATED_TXT.suffix + \".tmp\")\n",
    "    separator = \"\\n\\n------------------------------\\n\\n\"\n",
    "\n",
    "    logging.info(\"Merging %d TXT files into %s\", len(files), cfg.CONCATENATED_TXT.name)\n",
    "    written: List[Path] = []\n",
    "    try:\n",
    "        with tmp_out.open(\"w\", encoding=\"utf-8\", newline=\"\\n\") as out:\n",
    "            for idx, p in enumerate(files):\n",
    "                out.write(p.read_text(encoding=\"utf-8\", errors=\"ignore\"))\n",
    "                if idx < len(files) - 1:\n",
    "                    out.write(separator)\n",
    "                written.append(p)\n",
    "        os.replace(tmp_out, cfg.CONCATENATED_TXT)\n",
    "        logging.info(\"Merged successfully → %s\", cfg.CONCATENATED_TXT.resolve())\n",
    "        # Optionally delete per‑paper text files now that we have a single big file.\n",
    "        for p in written:\n",
    "            try:\n",
    "                p.unlink()\n",
    "            except Exception as e:\n",
    "                logging.warning(\"Failed to delete %s: %s\", p, e)\n",
    "    except Exception as e:\n",
    "        logging.error(\"Concatenation failed: %s\", e)\n",
    "        try:\n",
    "            tmp_out.unlink(missing_ok=True)\n",
    "        except Exception:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a223dfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------ Orchestration -------------------------- #\n",
    "\n",
    "def run_pipeline(cfg: Config = Config) -> None:\n",
    "    setup_logging(cfg.LOG_LEVEL)\n",
    "\n",
    "    ensure_dirs(cfg.BASE_DIR, cfg.PDF_DIR, cfg.TXT_DIR)\n",
    "\n",
    "    session = build_http_session(cfg)\n",
    "\n",
    "    # 1) Fetch fresh search metadata\n",
    "    fresh_papers = fetch_search_metadata(cfg.SEARCH_URL, session)\n",
    "\n",
    "    # Save the raw scrape for audit/debug\n",
    "    atomic_write_json(cfg.SCRAPE_OUTFILE, [asdict(p) for p in fresh_papers])\n",
    "    logging.info(\"Saved fresh scrape → %s\", cfg.SCRAPE_OUTFILE)\n",
    "\n",
    "    # 2) Load base metadata and diff\n",
    "    base_records = load_json_list(cfg.BASE_METADATA_FILE)\n",
    "    new_papers = compute_new_papers(base_records, fresh_papers)\n",
    "\n",
    "    # 3) Download PDFs for new papers\n",
    "    download_pdfs(new_papers, session, cfg)\n",
    "\n",
    "    # 4) Convert PDFs to TXT (and delete PDFs on success)\n",
    "    convert_pdfs_to_txt_and_cleanup(cfg)\n",
    "\n",
    "    # 5) Concatenate all TXT into one file\n",
    "    concatenate_txts(cfg)\n",
    "\n",
    "    # 6) Update base metadata (prepend new items for recency)\n",
    "    if new_papers:\n",
    "        updated = [asdict(p) for p in new_papers] + base_records\n",
    "        atomic_write_json(cfg.BASE_METADATA_FILE, updated)\n",
    "        logging.info(\n",
    "            \"Base metadata updated with %d new records → %s\",\n",
    "            len(new_papers),\n",
    "            cfg.BASE_METADATA_FILE,\n",
    "        )\n",
    "    else:\n",
    "        logging.info(\"No new metadata to add to base store.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b09647",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":   \n",
    "    local_tz = get_localzone()          # tzinfo\n",
    "    print(\"Scheduler timezone:\", local_tz)\n",
    "    scheduler = BlockingScheduler(timezone=local_tz)\n",
    "    scheduler.add_job(run_pipeline, CronTrigger(hour=7, minute=0, timezone=local_tz))  # Everyday at 7:00am in local timezone\n",
    "    scheduler.start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
